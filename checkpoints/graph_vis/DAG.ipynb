{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import re\n",
    "from graphviz import Digraph\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def func_aggregation(pipeline_func):\n",
    "    '''\n",
    "    This function is used for line execution with exec()\n",
    "    \n",
    "    args:\n",
    "        function strings after inspect\n",
    "    returns:\n",
    "        list of functionable strings for exec()\n",
    "        (input params, executable lines, output) \n",
    "    '''\n",
    "    func_str = inspect.getsource(pipeline_func)\n",
    "    res = [] # executables for return\n",
    "    stack_for_parent = [] # stack storing brackets for line integration\n",
    "    logs_of_parent = [] # logs of lines for concat\n",
    "#     convert function codes to list of strings\n",
    "    func_list = [item.strip() for item in func_str.split('\\n')]\n",
    "#     function args\n",
    "    func_args = [item.strip() for item in func_list[0].split('(')[1].rstrip('):').split(',')]\n",
    "    for item in func_list[1:]:\n",
    "        if not item:\n",
    "            continue\n",
    "        logs_of_parent.append(item)\n",
    "        for char in item:\n",
    "            if char == '(':\n",
    "                stack_for_parent.append('(')\n",
    "            if char == '[':\n",
    "                stack_for_parent.append('[')\n",
    "            if char == ')' and stack_for_parent[-1] == '(':\n",
    "                stack_for_parent.pop(-1)\n",
    "            if char == ']' and stack_for_parent[-1] == '[':\n",
    "                stack_for_parent.pop(-1)\n",
    "        if not stack_for_parent:\n",
    "            res.append(''.join(logs_of_parent))\n",
    "            logs_of_parent.clear()\n",
    "    return func_args, res[:-1], [item.strip() for item in res[-1].replace('return ', '').split(',')]\n",
    "\n",
    "def find_pd_lines(pipeline_func):\n",
    "    pd_lines = []\n",
    "    input_args , executable_list, _ = func_aggregation(pipeline_func)\n",
    "    for line in input_args:\n",
    "        exec(line)\n",
    "    for cur_line in executable_list:\n",
    "        exec(cur_line)\n",
    "        try: \n",
    "            if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "                pd_lines.append(cur_line)\n",
    "        except:\n",
    "            print(\"[ERROR]: error in finding pandas code\")\n",
    "    return pd_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DataFlowVertex:\n",
    "    def __init__(self, parent_vertices, name, operation, params):\n",
    "        self.parent_vertices = parent_vertices\n",
    "        self.name = name\n",
    "        self.operation = operation\n",
    "        self.params = params\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"(vertex_name={}: parent={}, op={}, params={})\".format(self.name, self.parent_vertices, self.operation, self.params)\n",
    "    \n",
    "    def display(self):\n",
    "        print(\"(vertex_name={}: parent={}, op={}, params={})\".format(self.name, self.parent_vertices, self.operation, self.params))\n",
    "\n",
    "def search_vertex_by_names(names, vertices_list):\n",
    "    result = []\n",
    "    for vertex in vertices_list:\n",
    "        if vertex.name in names:\n",
    "            result.append(vertex)\n",
    "    return result\n",
    "\n",
    "\n",
    "def pd_to_dataflow_graph(pipeline_func, parent_vertices=[]):\n",
    "    executable_list = find_pd_lines(pipeline_func)\n",
    "    graph = []\n",
    "    previous = []\n",
    "    \n",
    "    for line in executable_list:\n",
    "        var_name = line.split('=')[0].strip()\n",
    "\n",
    "        # match \".func_name(...)\"\n",
    "        pd_func = re.search('\\.\\s*([_a-z]*)\\s*\\(',line)  \n",
    "        if pd_func:\n",
    "            func_name = pd_func.group(1)\n",
    "            params = re.search('\\(([^\\)]*)\\)',line)  #\"(...)\"\n",
    "            \n",
    "            if params:\n",
    "                params = params.group(1).strip()\n",
    "\n",
    "                if func_name == 'read_csv': #df = pd.read_csv(path)\n",
    "                    vertex = DataFlowVertex(parent_vertices,var_name, func_name, params)\n",
    "                    previous.append(vertex)\n",
    "\n",
    "                elif func_name in ['join','merge','concat']:\n",
    "                    if func_name == 'concat': #df_new = pd.concat([df1,df2],keys=[])\n",
    "                        df_names = [name.strip() for name in params.strip('[]').split(',')]\n",
    "\n",
    "                    else: # df_new = df1.join/merge(df2,on='...',how='...')\n",
    "                        df_names = [re.split(\"[.=]\",line)[1].strip(), params.split(',')[0].strip()]      \n",
    "                    parent_vertices = search_vertex_by_names(df_names, graph) #search in graph by df_names\n",
    "                    vertex = DataFlowVertex(previous, var_name, func_name, params) #TODO vertex name?\n",
    "                    previous = [vertex]\n",
    "\n",
    "                else:\n",
    "                    vertex = DataFlowVertex(previous, func_name+' '+params, func_name, params)\n",
    "                    previous = [vertex]\n",
    "\n",
    "\n",
    "        # filter operation: \"df[[cols]]\", \"df[condition]\",\"df.loc[]\",\"df.iloc[]\"\n",
    "        else:\n",
    "            is_filter = re.search('\\[([^\\[\\]]+)\\]',line) #\"[...]\"\n",
    "            if is_filter:\n",
    "                filter_cond = is_filter.group(1)\n",
    "                vertex = DataFlowVertex(previous, 'select '+filter_cond, 'filter', filter_cond)\n",
    "                previous = [vertex]\n",
    "\n",
    "        graph.append(vertex)\n",
    "            \n",
    "    return graph, previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sklearn --> Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topo_sort(graph):\n",
    "    adjacency_list = {vertex.name: [] for vertex in graph}\n",
    "    visited = {vertex.name: False for vertex in graph}\n",
    "\n",
    "    for vertex in graph:\n",
    "        for parent_vertex in vertex.parent_vertices:\n",
    "            adjacency_list[parent_vertex.name].append(vertex.name)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    def toposort(vertex_name, adjacency_list, visited, output):\n",
    "        visited[vertex_name] = True\n",
    "        for child_name in adjacency_list[vertex_name]:\n",
    "            if not visited[child_name]:\n",
    "                toposort(child_name, adjacency_list, visited, output)\n",
    "        output.append(vertex_name)\n",
    "\n",
    "    for vertex_name in adjacency_list.keys():\n",
    "        if not visited[vertex_name]:\n",
    "            toposort(vertex_name, adjacency_list, visited, output)\n",
    "\n",
    "    output.reverse()\n",
    "\n",
    "    vertices_by_name = {vertex.name: vertex for vertex in graph}\n",
    "\n",
    "    sorted_graph = []\n",
    "    for vertex_name in output:\n",
    "        sorted_graph.append(vertices_by_name[vertex_name])\n",
    "    return sorted_graph\n",
    "\n",
    "\n",
    "def find_sink(graph):\n",
    "    sorted_graph = topo_sort(graph)\n",
    "    return sorted_graph[-1]\n",
    "\n",
    "def pipeline_to_dataflow_graph(pipeline, name_prefix='', parent_vertices=[]):\n",
    "    graph = []\n",
    "    parent_vertices_for_current_step = parent_vertices\n",
    "    parent_vertices_for_next_step = []\n",
    "\n",
    "    for step_name, component in pipeline.steps:\n",
    "        component_class_name = component.__class__.__name__\n",
    "\n",
    "        if component_class_name == 'ColumnTransformer':\n",
    "            for transformer_prefix, transformer_component, columns in component.transformers:\n",
    "                for column in columns:\n",
    "                    name = name_prefix + step_name + '__' + transformer_prefix + \"__\" + column\n",
    "                    transformer_component_class_name = transformer_component.__class__.__name__\n",
    "\n",
    "                    if transformer_component_class_name == 'Pipeline':\n",
    "\n",
    "                        vertices_to_add = pipeline_to_dataflow_graph(transformer_component,\n",
    "                                                                     name + \"__\",\n",
    "                                                                     parent_vertices_for_current_step)\n",
    "\n",
    "                        for vertex in vertices_to_add:\n",
    "                            graph.append(vertex)\n",
    "\n",
    "                        parent_vertices_for_next_step.append(find_sink(vertices_to_add))\n",
    "\n",
    "                    else:\n",
    "                        vertex = DataFlowVertex(parent_vertices_for_current_step,\n",
    "                                                name_prefix + name,\n",
    "                                                transformer_component_class_name,\n",
    "                                               '')\n",
    "                        graph.append(vertex)\n",
    "                        parent_vertices_for_next_step.append(vertex)\n",
    "\n",
    "        else:\n",
    "            vertex = DataFlowVertex(parent_vertices_for_current_step,\n",
    "                                    name_prefix + step_name,\n",
    "                                    component_class_name,\n",
    "                                   '')\n",
    "            graph.append(vertex)\n",
    "            parent_vertices_for_next_step.append(vertex)\n",
    "\n",
    "        parent_vertices_for_current_step = parent_vertices_for_next_step.copy()\n",
    "        parent_vertices_for_next_step = []\n",
    "\n",
    "    return graph\n",
    "\n",
    "def sklearn_to_dataflow_graph(pipeline, parent_vertices=[]):\n",
    "    \n",
    "    graph = pipeline_to_dataflow_graph(pipeline, name_prefix='', parent_vertices=parent_vertices)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize(nested_graph):\n",
    "    dot = Digraph(comment='preprocessing_pipeline')\n",
    "\n",
    "    for node in nested_graph:\n",
    "        dot.node(node.name,label = node.name+',\\nop='+node.operation)\n",
    "        parents = node.parent_vertices \n",
    "        \n",
    "        for parent in parents:\n",
    "            dot.edge(parent.name, node.name)\n",
    "    \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) test on pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country  capital   inhabitant to_drop\n",
      "0    Germany   Berlin   82500000.0      hi\n",
      "1     France    Paris   66900000.0      hi\n",
      "2  Indonesia  Jakarta  255500000.0      hi\n",
      "   country capital    HDI\n",
      "0  Germany  Berlin  0.926\n",
      "1    Italy    Rome  0.897\n",
      "2    Spain  Madrid  0.844\n",
      "3  Austria  Vienna  0.893\n"
     ]
    }
   ],
   "source": [
    "###test dfs\n",
    "countries = ['Germany', 'France', 'Indonesia']\n",
    "inhabitants = [82.5 * 10**6, 66.9 * 10**6, 255.5 * 10**6]\n",
    "capitals = ['Berlin', 'Paris', 'Jakarta']\n",
    "to_drop = ['hi','hi','hi']\n",
    "\n",
    "df1 = pd.DataFrame({'country': countries,\n",
    "                    'inhabitant': inhabitants,\n",
    "                    'capital': capitals,\n",
    "                   'to_drop': to_drop})\n",
    "df1 = df1[['country', 'capital', 'inhabitant','to_drop']]\n",
    "print(df1)\n",
    "\n",
    "countries = ['Germany', 'Italy', 'Spain', 'Austria']\n",
    "capitals = ['Berlin', 'Rome', 'Madrid', 'Vienna']\n",
    "hdis = [0.926, 0.897, 0.844, 0.893]\n",
    "df2 = pd.DataFrame({'country': countries,\n",
    "                    'capital': capitals,\n",
    "                    'HDI': hdis})\n",
    "df2 = df2[['country', 'capital', 'HDI']]\n",
    "print(df2)\n",
    "\n",
    "df1.to_csv ('../data/inhabitants.csv', index = None, header=True)\n",
    "df2.to_csv ('../data/country.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(vertex_name=select df.HDI >= 0.9: parent=[(vertex_name=select 'country','HDI': parent=[(vertex_name=drop ['to_drop'],axis=1: parent=[(vertex_name=dropna : parent=[(vertex_name=concat_df: parent=[(vertex_name=inhabitants: parent=[], op=read_csv, params='../data/inhabitants.csv'), (vertex_name=country: parent=[], op=read_csv, params='../data/country.csv')], op=concat, params=[inhabitants, country])], op=dropna, params=)], op=drop, params=['to_drop'],axis=1)], op=filter, params='country','HDI')], op=filter, params=df.HDI >= 0.9)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"274pt\" height=\"510pt\"\n",
       " viewBox=\"0.00 0.00 274.45 510.44\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 506.4407)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-506.4407 270.4508,-506.4407 270.4508,4 -4,4\"/>\n",
       "<!-- inhabitants -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>inhabitants</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"62.2254\" cy=\"-475.5706\" rx=\"62.4516\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.2254\" y=\"-479.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inhabitants,</text>\n",
       "<text text-anchor=\"middle\" x=\"62.2254\" y=\"-464.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=read_csv</text>\n",
       "</g>\n",
       "<!-- concat_df -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>concat_df</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-385.8305\" rx=\"53.066\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-389.6305\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">concat_df,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-374.6305\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=concat</text>\n",
       "</g>\n",
       "<!-- inhabitants&#45;&gt;concat_df -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inhabitants&#45;&gt;concat_df</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M82.3812,-450.0948C90.0157,-440.4452 98.8226,-429.3137 106.9392,-419.0549\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"109.805,-421.0735 113.2649,-411.0595 104.3154,-416.7302 109.805,-421.0735\"/>\n",
       "</g>\n",
       "<!-- country -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>country</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"204.2254\" cy=\"-475.5706\" rx=\"62.4516\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.2254\" y=\"-479.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">country,</text>\n",
       "<text text-anchor=\"middle\" x=\"204.2254\" y=\"-464.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=read_csv</text>\n",
       "</g>\n",
       "<!-- country&#45;&gt;concat_df -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>country&#45;&gt;concat_df</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.0696,-450.0948C176.4351,-440.4452 167.6282,-429.3137 159.5116,-419.0549\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"162.1354,-416.7302 153.1859,-411.0595 156.6457,-421.0735 162.1354,-416.7302\"/>\n",
       "</g>\n",
       "<!-- dropna  -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>dropna </title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-296.0904\" rx=\"54.3945\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-299.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropna ,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-284.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=dropna</text>\n",
       "</g>\n",
       "<!-- concat_df&#45;&gt;dropna  -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>concat_df&#45;&gt;dropna </title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-358.9055C133.2254,-350.8306 133.2254,-341.8273 133.2254,-333.2164\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-332.9964 133.2254,-322.9964 129.7255,-332.9964 136.7255,-332.9964\"/>\n",
       "</g>\n",
       "<!-- drop [&#39;to_drop&#39;],axis=1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>drop [&#39;to_drop&#39;],axis=1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-206.3503\" rx=\"104.8038\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-210.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">drop [&#39;to_drop&#39;],axis=1,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-195.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=drop</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;drop [&#39;to_drop&#39;],axis=1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;drop [&#39;to_drop&#39;],axis=1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-269.1654C133.2254,-261.0905 133.2254,-252.0872 133.2254,-243.4763\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-243.2562 133.2254,-233.2563 129.7255,-243.2563 136.7255,-243.2562\"/>\n",
       "</g>\n",
       "<!-- select &#39;country&#39;,&#39;HDI&#39; -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>select &#39;country&#39;,&#39;HDI&#39;</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-116.6102\" rx=\"96.7474\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">select &#39;country&#39;,&#39;HDI&#39;,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=filter</text>\n",
       "</g>\n",
       "<!-- drop [&#39;to_drop&#39;],axis=1&#45;&gt;select &#39;country&#39;,&#39;HDI&#39; -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>drop [&#39;to_drop&#39;],axis=1&#45;&gt;select &#39;country&#39;,&#39;HDI&#39;</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-179.4253C133.2254,-171.3504 133.2254,-162.3471 133.2254,-153.7362\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-153.5161 133.2254,-143.5162 129.7255,-153.5162 136.7255,-153.5161\"/>\n",
       "</g>\n",
       "<!-- select df.HDI &gt;= 0.9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>select df.HDI &gt;= 0.9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-26.8701\" rx=\"96.3333\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-30.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">select df.HDI &gt;= 0.9,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-15.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=filter</text>\n",
       "</g>\n",
       "<!-- select &#39;country&#39;,&#39;HDI&#39;&#45;&gt;select df.HDI &gt;= 0.9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>select &#39;country&#39;,&#39;HDI&#39;&#45;&gt;select df.HDI &gt;= 0.9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-89.6852C133.2254,-81.6102 133.2254,-72.607 133.2254,-63.9961\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-63.776 133.2254,-53.776 129.7255,-63.7761 136.7255,-63.776\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11a22d0d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_pd_1():\n",
    "    inhabitants = pd.read_csv('../data/inhabitants.csv')\n",
    "    country = pd.read_csv('../data/country.csv')\n",
    "    concat_df = pd.concat([inhabitants, country])\n",
    "    df = concat_df.dropna()\n",
    "    df = df.drop(['to_drop'],axis=1)\n",
    "    df = df[['country','HDI']]\n",
    "    df = df[df.HDI >= 0.9]\n",
    "    return df\n",
    "\n",
    "graph, parent_vertics = pd_to_dataflow_graph(test_pd_1)\n",
    "print(parent_vertics)\n",
    "visualize(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"274pt\" height=\"510pt\"\n",
       " viewBox=\"0.00 0.00 274.45 510.44\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 506.4407)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-506.4407 270.4508,-506.4407 270.4508,4 -4,4\"/>\n",
       "<!-- inhabitants -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>inhabitants</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"62.2254\" cy=\"-475.5706\" rx=\"62.4516\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.2254\" y=\"-479.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">inhabitants,</text>\n",
       "<text text-anchor=\"middle\" x=\"62.2254\" y=\"-464.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=read_csv</text>\n",
       "</g>\n",
       "<!-- merge_df -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>merge_df</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-385.8305\" rx=\"52.1524\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-389.6305\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">merge_df,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-374.6305\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=merge</text>\n",
       "</g>\n",
       "<!-- inhabitants&#45;&gt;merge_df -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>inhabitants&#45;&gt;merge_df</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M82.3812,-450.0948C90.0804,-440.3634 98.972,-429.125 107.1454,-418.7942\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.05,-420.7639 113.5098,-410.7499 104.5603,-416.4206 110.05,-420.7639\"/>\n",
       "</g>\n",
       "<!-- country -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>country</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"204.2254\" cy=\"-475.5706\" rx=\"62.4516\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"204.2254\" y=\"-479.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">country,</text>\n",
       "<text text-anchor=\"middle\" x=\"204.2254\" y=\"-464.3706\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=read_csv</text>\n",
       "</g>\n",
       "<!-- country&#45;&gt;merge_df -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>country&#45;&gt;merge_df</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.0696,-450.0948C176.3704,-440.3634 167.4788,-429.125 159.3054,-418.7942\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"161.8905,-416.4206 152.941,-410.7499 156.4008,-420.7639 161.8905,-416.4206\"/>\n",
       "</g>\n",
       "<!-- dropna  -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>dropna </title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-296.0904\" rx=\"54.3945\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-299.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropna ,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-284.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=dropna</text>\n",
       "</g>\n",
       "<!-- merge_df&#45;&gt;dropna  -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>merge_df&#45;&gt;dropna </title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-358.9055C133.2254,-350.8306 133.2254,-341.8273 133.2254,-333.2164\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-332.9964 133.2254,-322.9964 129.7255,-332.9964 136.7255,-332.9964\"/>\n",
       "</g>\n",
       "<!-- drop [&#39;to_drop&#39;],axis=1 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>drop [&#39;to_drop&#39;],axis=1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-206.3503\" rx=\"104.8038\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-210.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">drop [&#39;to_drop&#39;],axis=1,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-195.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=drop</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;drop [&#39;to_drop&#39;],axis=1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;drop [&#39;to_drop&#39;],axis=1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-269.1654C133.2254,-261.0905 133.2254,-252.0872 133.2254,-243.4763\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-243.2562 133.2254,-233.2563 129.7255,-243.2563 136.7255,-243.2562\"/>\n",
       "</g>\n",
       "<!-- select &#39;country&#39;,&#39;HDI&#39; -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>select &#39;country&#39;,&#39;HDI&#39;</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-116.6102\" rx=\"96.7474\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">select &#39;country&#39;,&#39;HDI&#39;,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=filter</text>\n",
       "</g>\n",
       "<!-- drop [&#39;to_drop&#39;],axis=1&#45;&gt;select &#39;country&#39;,&#39;HDI&#39; -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>drop [&#39;to_drop&#39;],axis=1&#45;&gt;select &#39;country&#39;,&#39;HDI&#39;</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-179.4253C133.2254,-171.3504 133.2254,-162.3471 133.2254,-153.7362\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-153.5161 133.2254,-143.5162 129.7255,-153.5162 136.7255,-153.5161\"/>\n",
       "</g>\n",
       "<!-- select df.HDI &gt;= 0.9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>select df.HDI &gt;= 0.9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.2254\" cy=\"-26.8701\" rx=\"96.3333\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-30.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">select df.HDI &gt;= 0.9,</text>\n",
       "<text text-anchor=\"middle\" x=\"133.2254\" y=\"-15.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=filter</text>\n",
       "</g>\n",
       "<!-- select &#39;country&#39;,&#39;HDI&#39;&#45;&gt;select df.HDI &gt;= 0.9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>select &#39;country&#39;,&#39;HDI&#39;&#45;&gt;select df.HDI &gt;= 0.9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.2254,-89.6852C133.2254,-81.6102 133.2254,-72.607 133.2254,-63.9961\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.7255,-63.776 133.2254,-53.776 129.7255,-63.7761 136.7255,-63.776\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x11a1fd310>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_pd_2():\n",
    "    inhabitants = pd.read_csv('../data/inhabitants.csv')\n",
    "    country = pd.read_csv('../data/country.csv')\n",
    "    merge_df = inhabitants.merge(country, on='country', how='inner')\n",
    "    df = merge_df.dropna()\n",
    "    df = df.drop(['to_drop'],axis=1)\n",
    "    df = df[['country','HDI']]\n",
    "    df = df[df.HDI >= 0.9]\n",
    "    return df\n",
    "\n",
    "graph, parent_vertics = pd_to_dataflow_graph(test_pd_2)\n",
    "visualize(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn + pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def pipeline_test_hwk3_3(f_path = '../pipelines/adult-sample.csv', a = 0):\n",
    "   \n",
    "    raw_data = pd.read_csv(f_path, na_values='?')   \n",
    "    data = raw_data.dropna()     \n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    feature_transformation = sklearn.compose.ColumnTransformer(transformers=[\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "        \n",
    "    income_pipeline = Pipeline([\n",
    "      ('features', feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "    \n",
    "    return income_pipeline\n",
    "\n",
    "def pipeline_test_hwk3_4(f_path = 'data/adult-sample.csv', a = 0):\n",
    "    raw_data = pd.read_csv(f_path, na_values='?')    #\n",
    "    data = raw_data.dropna()     #\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    nested_categorical_feature_transformation = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    nested_feature_transformation = ColumnTransformer(transformers=[\n",
    "        ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "    nested_pipeline = Pipeline([\n",
    "      ('features', nested_feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    return nested_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adult_pipeline_normal(f_path = '../pipelines/adult-sample_missing.csv'):\n",
    "    raw_data = pd.read_csv(f_path, na_values='?')\n",
    "    data = raw_data.dropna()\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    nested_categorical_feature_transformation = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    nested_feature_transformation = ColumnTransformer(transformers=[\n",
    "        ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "    nested_pipeline = Pipeline([\n",
    "      ('features', nested_feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    return nested_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_vertices [(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)]\n",
      "==========\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dropna '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7e916c8f0a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=========='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msklearn_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madult_pipeline_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msklearn_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_to_dataflow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparent_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-393817b1e0c0>\u001b[0m in \u001b[0;36msklearn_to_dataflow_graph\u001b[0;34m(pipeline, parent_vertices)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msklearn_to_dataflow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_vertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_to_dataflow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_vertices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-393817b1e0c0>\u001b[0m in \u001b[0;36mpipeline_to_dataflow_graph\u001b[0;34m(pipeline, name_prefix, parent_vertices)\u001b[0m\n\u001b[1;32m     57\u001b[0m                             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                         \u001b[0mparent_vertices_for_next_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices_to_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-393817b1e0c0>\u001b[0m in \u001b[0;36mfind_sink\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0msorted_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopo_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-393817b1e0c0>\u001b[0m in \u001b[0;36mtopo_sort\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparent_vertex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvertex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_vertices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0madjacency_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparent_vertex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dropna '"
     ]
    }
   ],
   "source": [
    "pd_graph, parent_vertices = pd_to_dataflow_graph(adult_pipeline_normal)\n",
    "\n",
    "print('parent_vertices', parent_vertices)\n",
    "print('==========')\n",
    "sklearn_pipeline = adult_pipeline_normal()\n",
    "sklearn_graph = sklearn_to_dataflow_graph(sklearn_pipeline,parent_vertices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?'),\n",
       " (vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=),\n",
       " (vertex_name=features__categorical__education: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=OneHotEncoder, params=),\n",
       " (vertex_name=features__categorical__workclass: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=OneHotEncoder, params=),\n",
       " (vertex_name=features__numeric__age: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=StandardScaler, params=),\n",
       " (vertex_name=features__numeric__hours-per-week: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=StandardScaler, params=),\n",
       " (vertex_name=classifier: parent=[(vertex_name=features__categorical__education: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=OneHotEncoder, params=), (vertex_name=features__categorical__workclass: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=OneHotEncoder, params=), (vertex_name=features__numeric__age: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=StandardScaler, params=), (vertex_name=features__numeric__hours-per-week: parent=[(vertex_name=dropna : parent=[(vertex_name=raw_data: parent=[], op=read_csv, params=f_path, na_values='?')], op=dropna, params=)], op=StandardScaler, params=)], op=DecisionTreeClassifier, params=)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_graph.extend(sklearn_graph)\n",
    "pd_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1197pt\" height=\"331pt\"\n",
       " viewBox=\"0.00 0.00 1196.88 330.96\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 326.9605)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-326.9605 1192.8843,-326.9605 1192.8843,4 -4,4\"/>\n",
       "<!-- raw_data -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>raw_data</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"600.0782\" cy=\"-296.0904\" rx=\"62.4516\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-299.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">raw_data,</text>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-284.8904\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=read_csv</text>\n",
       "</g>\n",
       "<!-- dropna  -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>dropna </title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"600.0782\" cy=\"-206.3503\" rx=\"54.3945\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-210.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dropna ,</text>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-195.1503\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=dropna</text>\n",
       "</g>\n",
       "<!-- raw_data&#45;&gt;dropna  -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>raw_data&#45;&gt;dropna </title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M600.0782,-269.1654C600.0782,-261.0905 600.0782,-252.0872 600.0782,-243.4763\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"603.5783,-243.2562 600.0782,-233.2563 596.5783,-243.2563 603.5783,-243.2562\"/>\n",
       "</g>\n",
       "<!-- features__categorical__education -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>features__categorical__education</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"147.0782\" cy=\"-116.6102\" rx=\"147.1565\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"147.0782\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">features__categorical__education,</text>\n",
       "<text text-anchor=\"middle\" x=\"147.0782\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;features__categorical__education -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;features__categorical__education</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M549.432,-196.3172C479.1513,-182.3944 350.2144,-156.8518 257.2123,-138.4279\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.6833,-134.9533 247.1938,-136.4432 256.323,-141.8199 257.6833,-134.9533\"/>\n",
       "</g>\n",
       "<!-- features__categorical__workclass -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>features__categorical__workclass</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"461.0782\" cy=\"-116.6102\" rx=\"148.4848\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"461.0782\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">features__categorical__workclass,</text>\n",
       "<text text-anchor=\"middle\" x=\"461.0782\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=OneHotEncoder</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;features__categorical__workclass -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;features__categorical__workclass</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M566.7842,-184.8553C549.7202,-173.8385 528.6161,-160.2134 509.7632,-148.0418\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"511.5817,-145.0498 501.282,-142.5663 507.7849,-150.9307 511.5817,-145.0498\"/>\n",
       "</g>\n",
       "<!-- features__numeric__age -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>features__numeric__age</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"739.0782\" cy=\"-116.6102\" rx=\"111.9462\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"739.0782\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">features__numeric__age,</text>\n",
       "<text text-anchor=\"middle\" x=\"739.0782\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=StandardScaler</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;features__numeric__age -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;features__numeric__age</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M633.3722,-184.8553C650.7148,-173.6587 672.2305,-159.7678 691.3148,-147.4468\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"693.3856,-150.276 699.8885,-141.9116 689.5888,-144.3951 693.3856,-150.276\"/>\n",
       "</g>\n",
       "<!-- features__numeric__hours&#45;per&#45;week -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>features__numeric__hours&#45;per&#45;week</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1029.0782\" cy=\"-116.6102\" rx=\"159.6125\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"1029.0782\" y=\"-120.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">features__numeric__hours&#45;per&#45;week,</text>\n",
       "<text text-anchor=\"middle\" x=\"1029.0782\" y=\"-105.4102\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=StandardScaler</text>\n",
       "</g>\n",
       "<!-- dropna &#45;&gt;features__numeric__hours&#45;per&#45;week -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>dropna &#45;&gt;features__numeric__hours&#45;per&#45;week</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M650.5515,-195.7921C716.1321,-182.0736 832.4528,-157.7412 918.8956,-139.6587\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"919.6808,-143.0703 928.7523,-137.5968 918.2475,-136.2186 919.6808,-143.0703\"/>\n",
       "</g>\n",
       "<!-- classifier -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>classifier</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"600.0782\" cy=\"-26.8701\" rx=\"118.1737\" ry=\"26.7407\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-30.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">classifier,</text>\n",
       "<text text-anchor=\"middle\" x=\"600.0782\" y=\"-15.6701\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">op=DecisionTreeClassifier</text>\n",
       "</g>\n",
       "<!-- features__categorical__education&#45;&gt;classifier -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>features__categorical__education&#45;&gt;classifier</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M247.0518,-96.8052C322.4827,-81.8622 425.4791,-61.4585 500.7518,-46.5468\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"501.5887,-49.9491 510.7179,-44.5725 500.2283,-43.0826 501.5887,-49.9491\"/>\n",
       "</g>\n",
       "<!-- features__categorical__workclass&#45;&gt;classifier -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>features__categorical__workclass&#45;&gt;classifier</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M501.2828,-90.6536C517.1233,-80.4268 535.449,-68.5955 551.9835,-57.9206\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"553.9412,-60.8228 560.444,-52.4584 550.1444,-54.9419 553.9412,-60.8228\"/>\n",
       "</g>\n",
       "<!-- features__numeric__age&#45;&gt;classifier -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>features__numeric__age&#45;&gt;classifier</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M699.9891,-91.3737C683.8516,-80.9552 664.9894,-68.7775 648.0277,-57.8268\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"649.6552,-54.7116 639.3556,-52.228 645.8584,-60.5924 649.6552,-54.7116\"/>\n",
       "</g>\n",
       "<!-- features__numeric__hours&#45;per&#45;week&#45;&gt;classifier -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>features__numeric__hours&#45;per&#45;week&#45;&gt;classifier</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M928.7676,-95.6268C858.9069,-81.013 766.2367,-61.6278 697.1058,-47.1667\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"697.5873,-43.6918 687.0825,-45.07 696.154,-50.5435 697.5873,-43.6918\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1a1d2fc510>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize(pd_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
