{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trace \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFlowVertex:\n",
    "    def __init__(self, parent_vertices, name, operation):\n",
    "        self.parent_vertices = parent_vertices\n",
    "        self.name = name\n",
    "        self.operation = operation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}, (name={}, op={})\".format(self.parent_vertices, self.name, self.operation)\n",
    "\n",
    "\n",
    "def pipeline_to_dataflow_graph(pipeline):\n",
    "    graph = []\n",
    "    layer_graph = []\n",
    "    def helper(pipeline, name_prefix=[], parent_vertices=[]):\n",
    "        if 'ColumnTransformer' in str(type(pipeline)):\n",
    "            for step in pipeline.transformers:\n",
    "                for column_name in step[2]:\n",
    "                    helper(step[1], name_prefix+[column_name], parent_vertices)\n",
    "        elif 'Pipeline' in str(type(pipeline)):\n",
    "            layer_graph.clear()\n",
    "            for i, key in enumerate(pipeline.named_steps.keys()):\n",
    "                helper(pipeline.named_steps[key], name_prefix, parent_vertices+layer_graph)\n",
    "\n",
    "        else :\n",
    "            graph.append(DataFlowVertex(parent_vertices, ''.join(name_prefix), pipeline))\n",
    "            layer_graph.append(DataFlowVertex(parent_vertices, ''.join(name_prefix), pipeline))\n",
    "\n",
    "    helper(pipeline)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAG Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from functools import wraps\n",
    "\n",
    "# class TraceCalls(object):\n",
    "#     \"\"\" Use as a decorator on functions that should be traced. Several\n",
    "#         functions can be decorated - they will all be indented according\n",
    "#         to their call depth.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, stream=sys.stdout, indent_step=2, show_ret=False):\n",
    "#         self.stream = stream\n",
    "#         self.indent_step = indent_step\n",
    "#         self.show_ret = show_ret\n",
    "\n",
    "#         # This is a class attribute since we want to share the indentation\n",
    "#         # level between different traced functions, in case they call\n",
    "#         # each other.\n",
    "#         TraceCalls.cur_indent = 0\n",
    "\n",
    "#     def __call__(self, fn):\n",
    "#         @wraps(fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             indent = ' ' * TraceCalls.cur_indent\n",
    "#             argstr = ', '.join(\n",
    "#                 [repr(a) for a in args] +\n",
    "#                 [\"%s=%s\" % (a, repr(b)) for a, b in kwargs.items()])\n",
    "#             self.stream.write('%s%s(%s)\\n' % (indent, fn.__name__, argstr))\n",
    "\n",
    "#             TraceCalls.cur_indent += self.indent_step\n",
    "#             ret = fn(*args, **kwargs)\n",
    "#             TraceCalls.cur_indent -= self.indent_step\n",
    "\n",
    "#             if self.show_ret:\n",
    "#                 self.stream.write('%s--> %s\\n' % (indent, ret))\n",
    "#             return ret\n",
    "#         return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function pd_oprations at 0x10fba6b00>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @TraceCalls()\n",
    "def pd_oprations(ll):\n",
    "    a = pd.DataFrame(ll,columns = ['a'])\n",
    "    a = a.apply(lambda x: x)\n",
    "    return a\n",
    "\n",
    "pd_oprations([[1],[2],[3]])\n",
    "str(pd_oprations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test_3(f_path = 'adult-sample.csv', a = 0):\n",
    "   \n",
    "    raw_data = pd.read_csv(f_path, na_values='?')\n",
    "    data = raw_data.dropna()\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    feature_transformation = sklearn.compose.ColumnTransformer(transformers=[\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "        \n",
    "    income_pipeline = Pipeline([\n",
    "      ('features', feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "    \n",
    "    return income_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test_4(f_path = 'adult-sample.csv', a = 0):\n",
    "    raw_data = pd.read_csv(f_path, na_values='?')\n",
    "    data = raw_data.dropna()\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    nested_categorical_feature_transformation = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    nested_feature_transformation = ColumnTransformer(transformers=[\n",
    "        ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "    nested_pipeline = Pipeline([\n",
    "      ('features', nested_feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    return nested_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found module dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2           0 LOAD_CONST               1 (0)\n",
      "              2 LOAD_CONST               2 (('Pipeline',))\n",
      "              4 IMPORT_NAME              0 (sklearn.pipeline)\n",
      "              6 IMPORT_FROM              1 (Pipeline)\n",
      "              8 STORE_FAST               1 (Pipeline)\n",
      "             10 POP_TOP\n",
      "\n",
      "  3          12 LOAD_CONST               1 (0)\n",
      "             14 LOAD_CONST               3 (('DecisionTreeClassifier',))\n",
      "             16 IMPORT_NAME              2 (sklearn.tree)\n",
      "             18 IMPORT_FROM              3 (DecisionTreeClassifier)\n",
      "             20 STORE_FAST               2 (DecisionTreeClassifier)\n",
      "             22 POP_TOP\n",
      "\n",
      "  4          24 LOAD_CONST               1 (0)\n",
      "             26 LOAD_CONST               4 (('OneHotEncoder', 'StandardScaler', 'label_binarize'))\n",
      "             28 IMPORT_NAME              4 (sklearn.preprocessing)\n",
      "             30 IMPORT_FROM              5 (OneHotEncoder)\n",
      "             32 STORE_FAST               3 (OneHotEncoder)\n",
      "             34 IMPORT_FROM              6 (StandardScaler)\n",
      "             36 STORE_FAST               4 (StandardScaler)\n",
      "             38 IMPORT_FROM              7 (label_binarize)\n",
      "             40 STORE_FAST               5 (label_binarize)\n",
      "             42 POP_TOP\n",
      "\n",
      "  5          44 LOAD_CONST               1 (0)\n",
      "             46 LOAD_CONST               5 (('ColumnTransformer',))\n",
      "             48 IMPORT_NAME              8 (sklearn.compose)\n",
      "             50 IMPORT_FROM              9 (ColumnTransformer)\n",
      "             52 STORE_FAST               6 (ColumnTransformer)\n",
      "             54 POP_TOP\n",
      "\n",
      "  7          56 LOAD_GLOBAL             10 (pd)\n",
      "             58 LOAD_ATTR               11 (read_csv)\n",
      "             60 LOAD_FAST                0 (f_path)\n",
      "             62 LOAD_CONST               6 ('?')\n",
      "             64 LOAD_CONST               7 (('na_values',))\n",
      "             66 CALL_FUNCTION_KW         2\n",
      "             68 STORE_FAST               7 (raw_data)\n",
      "\n",
      "  8          70 LOAD_FAST                7 (raw_data)\n",
      "             72 LOAD_METHOD             12 (dropna)\n",
      "             74 CALL_METHOD              0\n",
      "             76 STORE_FAST               8 (data)\n",
      "\n",
      " 10          78 LOAD_FAST                5 (label_binarize)\n",
      "             80 LOAD_FAST                8 (data)\n",
      "             82 LOAD_CONST               8 ('income-per-year')\n",
      "             84 BINARY_SUBSCR\n",
      "             86 LOAD_CONST               9 ('>50K')\n",
      "             88 LOAD_CONST              10 ('<=50K')\n",
      "             90 BUILD_LIST               2\n",
      "             92 CALL_FUNCTION            2\n",
      "             94 STORE_FAST               9 (labels)\n",
      "\n",
      " 12          96 LOAD_GLOBAL             13 (sklearn)\n",
      "             98 LOAD_ATTR               14 (compose)\n",
      "            100 LOAD_ATTR                9 (ColumnTransformer)\n",
      "\n",
      " 13         102 LOAD_CONST              11 ('categorical')\n",
      "            104 LOAD_FAST                3 (OneHotEncoder)\n",
      "            106 LOAD_CONST              12 ('ignore')\n",
      "            108 LOAD_CONST              13 (('handle_unknown',))\n",
      "            110 CALL_FUNCTION_KW         1\n",
      "            112 LOAD_CONST              14 ('education')\n",
      "            114 LOAD_CONST              15 ('workclass')\n",
      "            116 BUILD_LIST               2\n",
      "            118 BUILD_TUPLE              3\n",
      "\n",
      " 14         120 LOAD_CONST              16 ('numeric')\n",
      "            122 LOAD_FAST                4 (StandardScaler)\n",
      "            124 CALL_FUNCTION            0\n",
      "            126 LOAD_CONST              17 ('age')\n",
      "            128 LOAD_CONST              18 ('hours-per-week')\n",
      "            130 BUILD_LIST               2\n",
      "            132 BUILD_TUPLE              3\n",
      "            134 BUILD_LIST               2\n",
      "            136 LOAD_CONST              19 (('transformers',))\n",
      "            138 CALL_FUNCTION_KW         1\n",
      "            140 STORE_FAST              10 (feature_transformation)\n",
      "\n",
      " 17         142 LOAD_FAST                1 (Pipeline)\n",
      "\n",
      " 18         144 LOAD_CONST              20 ('features')\n",
      "            146 LOAD_FAST               10 (feature_transformation)\n",
      "            148 BUILD_TUPLE              2\n",
      "\n",
      " 19         150 LOAD_CONST              21 ('classifier')\n",
      "            152 LOAD_FAST                2 (DecisionTreeClassifier)\n",
      "            154 CALL_FUNCTION            0\n",
      "            156 BUILD_TUPLE              2\n",
      "            158 BUILD_LIST               2\n",
      "            160 CALL_FUNCTION            1\n",
      "            162 STORE_FAST              11 (income_pipeline)\n",
      "\n",
      " 21         164 LOAD_FAST               11 (income_pipeline)\n",
      "            166 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "saved = dis.dis(pipeline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use module inspect to convert function codes into Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func = inspect.getsource(pipeline_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def pipeline_test(f_path = 'adult-sample.csv', a = 0):\\n   \\n    raw_data = pd.read_csv(f_path, na_values='?')\\n    data = raw_data.dropna()\\n\\n    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\\n\\n    feature_transformation = sklearn.compose.ColumnTransformer(transformers=[\\n        ('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),\\n        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\\n    ])\\n        \\n    income_pipeline = Pipeline([\\n      ('features', feature_transformation),\\n      ('classifier', DecisionTreeClassifier())])\\n    \\n    return income_pipeline\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func_list = [item[4:].rstrip() for item in raw_func.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"pipeline_test(f_path = 'adult-sample.csv'):\",\n",
       " 'from sklearn.pipeline import Pipeline',\n",
       " 'from sklearn.tree import DecisionTreeClassifier',\n",
       " 'from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize',\n",
       " 'from sklearn.compose import ColumnTransformer',\n",
       " '',\n",
       " \"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " '',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " '',\n",
       " 'feature_transformation = sklearn.compose.ColumnTransformer(transformers=[',\n",
       " \"    ('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),\",\n",
       " \"    ('numeric', StandardScaler(), ['age', 'hours-per-week'])\",\n",
       " '])',\n",
       " '',\n",
       " 'income_pipeline = Pipeline([',\n",
       " \"  ('features', feature_transformation),\",\n",
       " \"  ('classifier', DecisionTreeClassifier())])\",\n",
       " '',\n",
       " 'return income_pipeline',\n",
       " '']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_func_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For_if_statement Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bracket_balance(func_list):\n",
    "#     res = []\n",
    "#     # func_args = raw_func_list[0].split('(')[1].rstrip('):')\n",
    "#     stack_for_parent = []\n",
    "#     logs_of_parent = []\n",
    "#     for item in func_list:\n",
    "#         logs_of_parent.append(item)\n",
    "#     for char in item:\n",
    "#         if char == '(':\n",
    "#             stack_for_parent.append('(')\n",
    "#         if char == '[':\n",
    "#             stack_for_parent.append('[')\n",
    "#         if char == ')' and stack_for_parent[-1] == '(':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#         if char == ']' and stack_for_parent[-1] == '[':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#     if not stack_for_parent:\n",
    "#         res.append(' '.join(logs_of_parent))\n",
    "#         logs_of_parent.clear()\n",
    "#     return res\n",
    "\n",
    "# logs_of_loop_if = []\n",
    "# for item in raw_func_list:\n",
    "#     if item[-1] == ':':\n",
    "#         logs_of_loop_if.append(item)\n",
    "#     if not logs_of_loop_if and item.startwith('    '):\n",
    "#         logs_of_loop_if.append(item)\n",
    "#     else:\n",
    "#         item.strip()\n",
    "#     logs_of_parent.append(item)\n",
    "#     for char in item:\n",
    "#         if char == '(':\n",
    "#             stack_for_parent.append('(')\n",
    "#         if char == '[':\n",
    "#             stack_for_parent.append('[')\n",
    "#         if char == ')' and stack_for_parent[-1] == '(':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#         if char == ']' and stack_for_parent[-1] == '[':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#     if not stack_for_parent:\n",
    "#         res.append(' '.join(logs_of_parent))\n",
    "#         logs_of_parent.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Function to Convert Function to Executable Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_aggregation(func_str):\n",
    "    '''\n",
    "    This function is used for line execution with exec()\n",
    "    \n",
    "    args:\n",
    "        function strings after inspect\n",
    "    returns:\n",
    "        list of functionable strings for exec()\n",
    "    '''\n",
    "    \n",
    "    res = [] # executables for return\n",
    "    stack_for_parent = [] # stack storing brackets for line integration\n",
    "    logs_of_parent = [] # logs of lines for concat\n",
    "#     convert function codes to list of strings\n",
    "    func_list = [item.strip() for item in func_str.split('\\n')]\n",
    "#     function args\n",
    "    func_args = [item.strip() for item in func_list[0].split('(')[1].rstrip('):').split(',')]\n",
    "    for item in func_list[1:]:\n",
    "        if not item:\n",
    "            continue\n",
    "        logs_of_parent.append(item)\n",
    "        for char in item:\n",
    "            if char == '(':\n",
    "                stack_for_parent.append('(')\n",
    "            if char == '[':\n",
    "                stack_for_parent.append('[')\n",
    "            if char == ')' and stack_for_parent[-1] == '(':\n",
    "                stack_for_parent.pop(-1)\n",
    "            if char == ']' and stack_for_parent[-1] == '[':\n",
    "                stack_for_parent.pop(-1)\n",
    "        if not stack_for_parent:\n",
    "            res.append(''.join(logs_of_parent))\n",
    "            logs_of_parent.clear()\n",
    "    return func_args, res[:-1], [item.strip() for item in res[-1].replace('return ', '').split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_args, executable_list = func_aggregation(raw_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"f_path = 'adult-sample.csv'\", 'a = 0']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " \"feature_transformation = sklearn.compose.ColumnTransformer(transformers=[('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\",\n",
       " \"income_pipeline = Pipeline([('features', feature_transformation),('classifier', DecisionTreeClassifier())])\"]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_args:\n",
    "    exec(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['race', 'occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_it = iter(executable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "for _ in range(4):\n",
    "    cur_line = next(exec_it)\n",
    "    exec(cur_line)\n",
    "    try: \n",
    "        if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "            print('yes')\n",
    "            target_df = cur_line.split('=')[0].strip()\n",
    "            exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")\n",
    "        else:\n",
    "            exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")\n",
    "    except:\n",
    "        exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 3 debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"feature_transformation = sklearn.compose.ColumnTransformer(transformers=[('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar = cur_line.split('=')[0].strip()\n",
    "str(eval(f\"type({tar})\"))==\"<class 'pandas.core.frame.DataFrame'>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_graph = pipeline_to_dataflow_graph(income_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], (name=features__categorical__education, op=OneHotEncoder),\n",
       " [], (name=features__categorical__workclass, op=OneHotEncoder),\n",
       " [], (name=features__numeric__age, op=StandardScaler),\n",
       " [], (name=features__numeric__hours-per-week, op=StandardScaler),\n",
       " [[], (name=features__categorical__education, op=OneHotEncoder), [], (name=features__categorical__workclass, op=OneHotEncoder), [], (name=features__numeric__age, op=StandardScaler), [], (name=features__numeric__hours-per-week, op=StandardScaler)], (name=classifier, op=DecisionTreeClassifier)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 4 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func_4 = inspect.getsource(pipeline_test_4)\n",
    "\n",
    "input_args, executable_list, outputs = func_aggregation(raw_func_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_args:\n",
    "    exec(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " \"nested_categorical_feature_transformation = Pipeline(steps=[('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),('encode', OneHotEncoder(handle_unknown='ignore'))])\",\n",
       " \"nested_feature_transformation = ColumnTransformer(transformers=[('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\",\n",
       " \"nested_pipeline = Pipeline([('features', nested_feature_transformation),('classifier', DecisionTreeClassifier())])\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nested_pipeline']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['race', 'occupation', 'education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race          100\n",
      "occupation     94\n",
      "education     100\n",
      "dtype: int64\n",
      "Inpected raw_data = pd.read_csv(f_path, na_values='?')\n",
      "-------------------------------------------------------\n",
      "\n",
      "Count Changed in race with value -8\n",
      "Count Changed in occupation with value -2\n",
      "Count Changed in education with value -8\n",
      "race          92\n",
      "occupation    92\n",
      "education     92\n",
      "dtype: int64\n",
      "Inpected data = raw_data.dropna()\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev = None\n",
    "for cur_line in executable_list:\n",
    "    print_bool = False\n",
    "    exec(cur_line)\n",
    "    try: \n",
    "        if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "            target_df = cur_line.split('=')[0].strip()\n",
    "            count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "            if prev is not None:\n",
    "                for col in target_col:\n",
    "                    dif = count_log[col] - prev[col]\n",
    "                    if dif != 0:\n",
    "                        print(f'Count Changed in {col} with value {dif}')\n",
    "                        print_bool = True\n",
    "            else:\n",
    "                print_bool = True\n",
    "            \n",
    "            if print_bool:\n",
    "                pprint.pprint(count_log)\n",
    "                print(f'Inpected {cur_line}')\n",
    "                print('-------------------------------------------------------')\n",
    "                print()\n",
    "            prev = count_log\n",
    "            \n",
    "        elif str(eval(f\"type({cur_line.split('=')[0].strip()})\")).startswith(\"<class 'sklearn\"):\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "            for col in target_col:\n",
    "                dif = count_log[col] - prev[col] \n",
    "                if dif != 0:\n",
    "                    print(f'Count Changed in {col} with value {dif}')\n",
    "                    print_bool = True\n",
    "            if print_bool:\n",
    "                pprint.pprint(count_log)\n",
    "                print(f'Inspected {cur_line}')\n",
    "                print('-------------------------------------------------------')\n",
    "                print()\n",
    "            prev = count_log\n",
    "            \n",
    "    except:\n",
    "#         print(f'inspecting {cur_line}')\n",
    "#         exec(f\"pprint.pprint({target_df}[{target_col}].count())\")\n",
    "#         print()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsf\n"
     ]
    }
   ],
   "source": [
    "if not 0:\n",
    "    print('dsf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_graph = pipeline_to_dataflow_graph(eval(f'{outputs[0]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], (name=education, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)),\n",
       " [[], (name=education, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=education, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)),\n",
       " [], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)),\n",
       " [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=workclass, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)),\n",
       " [], (name=age, op=StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " [], (name=hours-per-week, op=StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)), [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=workclass, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)), [], (name=age, op=StandardScaler(copy=True, with_mean=True, with_std=True)), [], (name=hours-per-week, op=StandardScaler(copy=True, with_mean=True, with_std=True))], (name=, op=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count          92\n",
      "unique         12\n",
      "top       HS-grad\n",
      "freq           29\n",
      "count                                                    92\n",
      "unique                                                    1\n",
      "top         (0, 11)\\t1.0\\n  (1, 8)\\t1.0\\n  (2, 7)\\t1.0\\n...\n",
      "freq                                                     92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for item in nested_graph:\n",
    "    if item.name in target_col:\n",
    "        eval(target_df)[item.name] = item.operation.fit_transform(eval(target_df)[item.name].values.reshape(-1,1))\n",
    "        print(eval(target_df)[item.name].describe().to_string())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_ver(pipeline_to_test, target_col = ['race', 'occupation', 'education']):\n",
    "    \n",
    "    def func_aggregation(func_str):\n",
    "        '''\n",
    "        This function is used for line execution with exec()\n",
    "\n",
    "        args:\n",
    "            function strings after inspect\n",
    "        returns:\n",
    "            list of functionable strings for exec()\n",
    "        '''\n",
    "\n",
    "        res = [] # executables for return\n",
    "        stack_for_parent = [] # stack storing brackets for line integration\n",
    "        logs_of_parent = [] # logs of lines for concat\n",
    "    #     convert function codes to list of strings\n",
    "        func_list = [item.strip() for item in func_str.split('\\n')]\n",
    "    #     function args\n",
    "        func_args = [item.strip() for item in func_list[0].split('(')[1].rstrip('):').split(',')]\n",
    "        for item in func_list[1:]:\n",
    "            if not item:\n",
    "                continue\n",
    "            logs_of_parent.append(item)\n",
    "            for char in item:\n",
    "                if char == '(':\n",
    "                    stack_for_parent.append('(')\n",
    "                if char == '[':\n",
    "                    stack_for_parent.append('[')\n",
    "                if char == ')' and stack_for_parent[-1] == '(':\n",
    "                    stack_for_parent.pop(-1)\n",
    "                if char == ']' and stack_for_parent[-1] == '[':\n",
    "                    stack_for_parent.pop(-1)\n",
    "            if not stack_for_parent:\n",
    "                res.append(''.join(logs_of_parent))\n",
    "                logs_of_parent.clear()\n",
    "        return func_args, res[:-1], [item.strip() for item in res[-1].replace('return ', '').split(',')]\n",
    "    \n",
    "    \n",
    "    raw_func = inspect.getsource(pipeline_to_test)\n",
    "\n",
    "    input_args, executable_list, outputs = func_aggregation(raw_func)\n",
    "    \n",
    "    for line in input_args:\n",
    "        exec(line)\n",
    "    \n",
    "    print()\n",
    "    print('####################### Start Pandas Opeation #######################')\n",
    "    print()\n",
    "    \n",
    "    prev = None\n",
    "    for cur_line in executable_list:\n",
    "        print_bool = False\n",
    "        exec(cur_line)\n",
    "        try: \n",
    "            if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "                target_df = cur_line.split('=')[0].strip()\n",
    "                count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "                if prev is not None:\n",
    "                    for col in target_col:\n",
    "                        dif = count_log[col] - prev[col]\n",
    "                        if dif != 0:\n",
    "                            print(f'Count Changed in {col} with value {dif}')\n",
    "                            print_bool = True\n",
    "                else:\n",
    "                    print_bool = True\n",
    "\n",
    "                if print_bool:\n",
    "                    pprint.pprint(count_log)\n",
    "                    print(f'Inpected {cur_line}')\n",
    "                    print('-------------------------------------------------------')\n",
    "                    print()\n",
    "                prev = count_log\n",
    "\n",
    "            elif str(eval(f\"type({cur_line.split('=')[0].strip()})\")).startswith(\"<class 'sklearn\"):\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "                for col in target_col:\n",
    "                    dif = count_log[col] - prev[col] \n",
    "                    if dif != 0:\n",
    "                        print(f'Count Changed in {col} with value {dif}')\n",
    "                        print_bool = True\n",
    "                if print_bool:\n",
    "                    pprint.pprint(count_log)\n",
    "                    print(f'Inspected {cur_line}')\n",
    "                    print('-------------------------------------------------------')\n",
    "                    print()\n",
    "                prev = count_log\n",
    "\n",
    "        except:\n",
    "    #         print(f'inspecting {cur_line}')\n",
    "    #         exec(f\"pprint.pprint({target_df}[{target_col}].count())\")\n",
    "    #         print()\n",
    "            pass\n",
    "    \n",
    "    nested_graph = pipeline_to_dataflow_graph(eval(f'{outputs[0]}'))\n",
    "    \n",
    "    print()\n",
    "    print('####################### Start Sklearn Pipeline #######################')\n",
    "    print()\n",
    "    \n",
    "    for item in nested_graph:\n",
    "        if item.name in target_col:\n",
    "            eval(target_df)[item.name] = item.operation.fit_transform(eval(target_df)[item.name].values.reshape(-1,1))\n",
    "            print(f\"Operations {str(item.operation).split('(')[0]} on {item.name}\")\n",
    "            count_log = import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "\n",
    "def get_beam_dot(beam: Beam, plot_size=30):\n",
    "        \"\"\"Create pydot graph representation of the beam.\n",
    "        \"\"\"\n",
    "\n",
    "        graph = nx.DiGraph()\n",
    "        outputs = numpy.array([i.tolist() for i in beams[0].outputs])\n",
    "        bookkeep = numpy.array([i.tolist() for i in beams[0].bookkeep])\n",
    "        all_scores = numpy.array([i.tolist() for i in beams[0].all_scores])\n",
    "        \n",
    "        max_ts = outputs.shape[0]\n",
    "        \n",
    "        labels_dict = {}\n",
    "        node_color_map = []\n",
    "\n",
    "        for i in range(max_ts):\n",
    "            if i == 0:\n",
    "                # only one start\n",
    "                start_node = f\"t_{0}__hid_{0}__tok_{outputs[i][0]}__sc_{all_scores[i][0]}\"\n",
    "                #start_node = {\"time\":0, \"hypid\": 0, \"token\": outputs[i][0], \"score\": all_scores[i][0]}\n",
    "                graph.add_node(start_node)\n",
    "                labels_dict[start_node] = chat_dict.ind2word[outputs[i][0]]\n",
    "                node_color_map.append('aliceblue')\n",
    "                continue\n",
    "\n",
    "            for hypid, token in enumerate(outputs[i]): # go over each token on this level\n",
    "                backtrack_hypid = bookkeep[i-1][hypid]\n",
    "                backtracked_node = f\"t_{i-1}__hid_{backtrack_hypid}__tok_{outputs[i-1][backtrack_hypid]}__sc_{all_scores[i-1][backtrack_hypid]}\"\n",
    "                current_score = all_scores[i][hypid]\n",
    "                node = f\"t_{i}__hid_{hypid}__tok_{token}__sc_{current_score}\"\n",
    "                graph.add_node(node)\n",
    "                graph.add_edge(backtracked_node, node)\n",
    "\n",
    "                if token == 2:\n",
    "                    node_color_map.append('pink')\n",
    "                    labels_dict[node] = \"__end__\\n{:.{prec}f}\".format(current_score, prec=4)\n",
    "                else:\n",
    "                    node_color_map.append('aliceblue')\n",
    "                    labels_dict[node] = chat_dict.ind2word[token]\n",
    "\n",
    "        # same layout using matplotlib with no labels\n",
    "        plt.figure(figsize=(plot_size,plot_size))\n",
    "        plt.title('Beam tree')\n",
    "        pos =graphviz_layout(graph, prog='dot')\n",
    "        nx.draw(graph, pos, labels=labels_dict, with_labels=True, arrows=True, font_size=24, node_size=5000, font_color='black', alpha=1.0, node_color=node_color_map)\n",
    "            dif = count_log - prev[item.name]\n",
    "            if dif != 0:\n",
    "                print(f\"count now is {count_log}, dif is {dif}\")\n",
    "            else:\n",
    "                print('no changes')\n",
    "            prev[item.name] = count_log\n",
    "            print('-------------------------------------------------------')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################### Start Pandas Opeation #######################\n",
      "\n",
      "\n",
      "####################### Start Sklearn Pipeline #######################\n",
      "\n",
      "Operations SimpleImputer on education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ea56ca72c486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribe_ver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_test_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-da46c2095d93>\u001b[0m in \u001b[0;36mdescribe_ver\u001b[0;34m(pipeline_to_test, target_col)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Operations {str(item.operation).split('(')[0]} on {item.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mcount_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_log\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdif\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"count now is {count_log}, dif is {dif}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "describe_ver(pipeline_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
