{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trace \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, label_binarize\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import inspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "####################################################################################\n",
    "def test_pipeline():\n",
    "    train = pd.read_csv('loan_train.csv')\n",
    "    test = pd.read_csv('loan_test.csv')\n",
    "\n",
    "    # Loan_ID is not needed in training or prediction\n",
    "    train = train.drop('Loan_ID', axis=1)\n",
    "\n",
    "    X = train.drop('Loan_Status', axis=1)\n",
    "    y = train['Loan_Status']\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "    # do transformer on numeric & categorical data respectively\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    numeric_features = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = train.select_dtypes(include=['object']).drop(['Loan_Status'], axis=1).columns\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    # classifier\n",
    "    rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', RandomForestClassifier())])\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sink(graph):\n",
    "    sorted_graph = topo_sort(graph)\n",
    "    return sorted_graph[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_dataflow_graph(pipeline, name_prefix='', parent_vertices=[]):\n",
    "    graph = []\n",
    "    parent_vertices_for_current_step = parent_vertices\n",
    "    parent_vertices_for_next_step = []\n",
    "\n",
    "    for step_name, component in pipeline.steps:\n",
    "        component_class_name = component.__class__.__name__\n",
    "\n",
    "        if component_class_name == 'ColumnTransformer':\n",
    "            for transformer_prefix, transformer_component, columns in component.transformers:\n",
    "                for column in columns:\n",
    "                    name = name_prefix + step_name + '__' + transformer_prefix + \"__\" + column\n",
    "                    transformer_component_class_name = transformer_component.__class__.__name__\n",
    "\n",
    "                    if transformer_component_class_name == 'Pipeline':\n",
    "\n",
    "                        vertices_to_add = pipeline_to_dataflow_graph(transformer_component,\n",
    "                                                                     name + \"__\",\n",
    "                                                                     parent_vertices_for_current_step)\n",
    "\n",
    "                        for vertex in vertices_to_add:\n",
    "                            graph.append(vertex)\n",
    "\n",
    "                        parent_vertices_for_next_step.append(find_sink(vertices_to_add))\n",
    "\n",
    "                    else:\n",
    "                        vertex = DataFlowVertex(parent_vertices_for_current_step,\n",
    "                                                name_prefix + name,\n",
    "                                                transformer_component_class_name)\n",
    "                        graph.append(vertex)\n",
    "                        parent_vertices_for_next_step.append(vertex)\n",
    "\n",
    "        else:\n",
    "            vertex = DataFlowVertex(parent_vertices_for_current_step,\n",
    "                                    name_prefix + step_name,\n",
    "                                    component_class_name)\n",
    "            graph.append(vertex)\n",
    "            parent_vertices_for_next_step.append(vertex)\n",
    "\n",
    "        parent_vertices_for_current_step = parent_vertices_for_next_step.copy()\n",
    "        parent_vertices_for_next_step = []\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFlowVertex:\n",
    "    def __init__(self, parent_vertices, name, operation):\n",
    "        self.parent_vertices = parent_vertices\n",
    "        self.name = name\n",
    "        self.operation = operation\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{}, (name={}, op={})\".format(self.parent_vertices, self.name, self.operation)\n",
    "\n",
    "\n",
    "def pipeline_to_dataflow_graph(pipeline):\n",
    "    layer_graph = []\n",
    "    all_layers  = []\n",
    "    graph = []\n",
    "    # TODO Implement translation of the pipeline into a list of DataFlowVertex objects\n",
    "    def helper(pipeline, name_prefix=[], parent_vertices=[], all_layers=[], level = 0):\n",
    "        if 'ColumnTransformer' in str(type(pipeline)):\n",
    "            for step in pipeline.transformers:\n",
    "                for column_name in step[2]:\n",
    "                    helper(step[1], name_prefix+[step[0]]+[column_name], parent_vertices, all_layers, level)\n",
    "        elif 'Pipeline' in str(type(pipeline)):\n",
    "#             print(level)\n",
    "            if layer_graph:\n",
    "                all_layers+=layer_graph\n",
    "            layer_graph.clear()\n",
    "            for i, key in enumerate(pipeline.named_steps.keys()):\n",
    "                if level == 0:\n",
    "                    helper(pipeline.named_steps[key], name_prefix+[key], parent_vertices+all_layers+layer_graph, all_layers, level+1)\n",
    "                else:\n",
    "                    helper(pipeline.named_steps[key], name_prefix+[key], parent_vertices+layer_graph, all_layers, level+1)\n",
    "\n",
    "        else :\n",
    "#             global layer_graph \n",
    "            graph.append(DataFlowVertex(parent_vertices, '__'.join(name_prefix), str(pipeline).split('(')[0]))\n",
    "            layer_graph.append(DataFlowVertex(parent_vertices, '__'.join(name_prefix), str(pipeline).split('(')[0]))\n",
    "    helper(pipeline)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_test_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topo_sort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-221a0962750f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline_to_dataflow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-282-8956482b80a6>\u001b[0m in \u001b[0;36mpipeline_to_dataflow_graph\u001b[0;34m(pipeline, name_prefix, parent_vertices)\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                         \u001b[0mparent_vertices_for_next_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices_to_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-278-58543bde165d>\u001b[0m in \u001b[0;36mfind_sink\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msorted_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopo_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted_graph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topo_sort' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline_to_dataflow_graph(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('categorical',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('impute',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='most_frequent',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('encode',\n",
       "                                                                   OneHotEncoder(categorica...\n",
       "                                                  ['age', 'hours-per-week'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features=None,\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        presort=False, random_state=None,\n",
       "                                        splitter='best'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAG Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from functools import wraps\n",
    "\n",
    "# class TraceCalls(object):\n",
    "#     \"\"\" Use as a decorator on functions that should be traced. Several\n",
    "#         functions can be decorated - they will all be indented according\n",
    "#         to their call depth.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, stream=sys.stdout, indent_step=2, show_ret=False):\n",
    "#         self.stream = stream\n",
    "#         self.indent_step = indent_step\n",
    "#         self.show_ret = show_ret\n",
    "\n",
    "#         # This is a class attribute since we want to share the indentation\n",
    "#         # level between different traced functions, in case they call\n",
    "#         # each other.\n",
    "#         TraceCalls.cur_indent = 0\n",
    "\n",
    "#     def __call__(self, fn):\n",
    "#         @wraps(fn)\n",
    "#         def wrapper(*args, **kwargs):\n",
    "#             indent = ' ' * TraceCalls.cur_indent\n",
    "#             argstr = ', '.join(\n",
    "#                 [repr(a) for a in args] +\n",
    "#                 [\"%s=%s\" % (a, repr(b)) for a, b in kwargs.items()])\n",
    "#             self.stream.write('%s%s(%s)\\n' % (indent, fn.__name__, argstr))\n",
    "\n",
    "#             TraceCalls.cur_indent += self.indent_step\n",
    "#             ret = fn(*args, **kwargs)\n",
    "#             TraceCalls.cur_indent -= self.indent_step\n",
    "\n",
    "#             if self.show_ret:\n",
    "#                 self.stream.write('%s--> %s\\n' % (indent, ret))\n",
    "#             return ret\n",
    "#         return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function pd_oprations at 0x10fba6b00>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @TraceCalls()\n",
    "def pd_oprations(ll):\n",
    "    a = pd.DataFrame(ll,columns = ['a'])\n",
    "    a = a.apply(lambda x: x)\n",
    "    return a\n",
    "\n",
    "pd_oprations([[1],[2],[3]])\n",
    "str(pd_oprations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test_3(f_path = 'adult-sample.csv', a = 0):\n",
    "   \n",
    "    raw_data = pd.read_csv(f_path, na_values='?')\n",
    "    data = raw_data.dropna()\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    feature_transformation = ColumnTransformer(transformers=[\n",
    "        ('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "        \n",
    "    income_pipeline = Pipeline([\n",
    "      ('features', feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "    \n",
    "    return income_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test_4(f_path = 'adult-sample.csv', a = 0):\n",
    "    raw_data = pd.read_csv(f_path, na_values='?')\n",
    "    data = raw_data.dropna()\n",
    "\n",
    "    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\n",
    "\n",
    "    nested_categorical_feature_transformation = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    nested_feature_transformation = ColumnTransformer(transformers=[\n",
    "        ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\n",
    "        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\n",
    "    ])\n",
    "\n",
    "    nested_pipeline = Pipeline([\n",
    "      ('features', nested_feature_transformation),\n",
    "      ('classifier', DecisionTreeClassifier())])\n",
    "\n",
    "    return nested_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found module dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2           0 LOAD_CONST               1 (0)\n",
      "              2 LOAD_CONST               2 (('Pipeline',))\n",
      "              4 IMPORT_NAME              0 (sklearn.pipeline)\n",
      "              6 IMPORT_FROM              1 (Pipeline)\n",
      "              8 STORE_FAST               1 (Pipeline)\n",
      "             10 POP_TOP\n",
      "\n",
      "  3          12 LOAD_CONST               1 (0)\n",
      "             14 LOAD_CONST               3 (('DecisionTreeClassifier',))\n",
      "             16 IMPORT_NAME              2 (sklearn.tree)\n",
      "             18 IMPORT_FROM              3 (DecisionTreeClassifier)\n",
      "             20 STORE_FAST               2 (DecisionTreeClassifier)\n",
      "             22 POP_TOP\n",
      "\n",
      "  4          24 LOAD_CONST               1 (0)\n",
      "             26 LOAD_CONST               4 (('OneHotEncoder', 'StandardScaler', 'label_binarize'))\n",
      "             28 IMPORT_NAME              4 (sklearn.preprocessing)\n",
      "             30 IMPORT_FROM              5 (OneHotEncoder)\n",
      "             32 STORE_FAST               3 (OneHotEncoder)\n",
      "             34 IMPORT_FROM              6 (StandardScaler)\n",
      "             36 STORE_FAST               4 (StandardScaler)\n",
      "             38 IMPORT_FROM              7 (label_binarize)\n",
      "             40 STORE_FAST               5 (label_binarize)\n",
      "             42 POP_TOP\n",
      "\n",
      "  5          44 LOAD_CONST               1 (0)\n",
      "             46 LOAD_CONST               5 (('ColumnTransformer',))\n",
      "             48 IMPORT_NAME              8 (sklearn.compose)\n",
      "             50 IMPORT_FROM              9 (ColumnTransformer)\n",
      "             52 STORE_FAST               6 (ColumnTransformer)\n",
      "             54 POP_TOP\n",
      "\n",
      "  7          56 LOAD_GLOBAL             10 (pd)\n",
      "             58 LOAD_ATTR               11 (read_csv)\n",
      "             60 LOAD_FAST                0 (f_path)\n",
      "             62 LOAD_CONST               6 ('?')\n",
      "             64 LOAD_CONST               7 (('na_values',))\n",
      "             66 CALL_FUNCTION_KW         2\n",
      "             68 STORE_FAST               7 (raw_data)\n",
      "\n",
      "  8          70 LOAD_FAST                7 (raw_data)\n",
      "             72 LOAD_METHOD             12 (dropna)\n",
      "             74 CALL_METHOD              0\n",
      "             76 STORE_FAST               8 (data)\n",
      "\n",
      " 10          78 LOAD_FAST                5 (label_binarize)\n",
      "             80 LOAD_FAST                8 (data)\n",
      "             82 LOAD_CONST               8 ('income-per-year')\n",
      "             84 BINARY_SUBSCR\n",
      "             86 LOAD_CONST               9 ('>50K')\n",
      "             88 LOAD_CONST              10 ('<=50K')\n",
      "             90 BUILD_LIST               2\n",
      "             92 CALL_FUNCTION            2\n",
      "             94 STORE_FAST               9 (labels)\n",
      "\n",
      " 12          96 LOAD_GLOBAL             13 (sklearn)\n",
      "             98 LOAD_ATTR               14 (compose)\n",
      "            100 LOAD_ATTR                9 (ColumnTransformer)\n",
      "\n",
      " 13         102 LOAD_CONST              11 ('categorical')\n",
      "            104 LOAD_FAST                3 (OneHotEncoder)\n",
      "            106 LOAD_CONST              12 ('ignore')\n",
      "            108 LOAD_CONST              13 (('handle_unknown',))\n",
      "            110 CALL_FUNCTION_KW         1\n",
      "            112 LOAD_CONST              14 ('education')\n",
      "            114 LOAD_CONST              15 ('workclass')\n",
      "            116 BUILD_LIST               2\n",
      "            118 BUILD_TUPLE              3\n",
      "\n",
      " 14         120 LOAD_CONST              16 ('numeric')\n",
      "            122 LOAD_FAST                4 (StandardScaler)\n",
      "            124 CALL_FUNCTION            0\n",
      "            126 LOAD_CONST              17 ('age')\n",
      "            128 LOAD_CONST              18 ('hours-per-week')\n",
      "            130 BUILD_LIST               2\n",
      "            132 BUILD_TUPLE              3\n",
      "            134 BUILD_LIST               2\n",
      "            136 LOAD_CONST              19 (('transformers',))\n",
      "            138 CALL_FUNCTION_KW         1\n",
      "            140 STORE_FAST              10 (feature_transformation)\n",
      "\n",
      " 17         142 LOAD_FAST                1 (Pipeline)\n",
      "\n",
      " 18         144 LOAD_CONST              20 ('features')\n",
      "            146 LOAD_FAST               10 (feature_transformation)\n",
      "            148 BUILD_TUPLE              2\n",
      "\n",
      " 19         150 LOAD_CONST              21 ('classifier')\n",
      "            152 LOAD_FAST                2 (DecisionTreeClassifier)\n",
      "            154 CALL_FUNCTION            0\n",
      "            156 BUILD_TUPLE              2\n",
      "            158 BUILD_LIST               2\n",
      "            160 CALL_FUNCTION            1\n",
      "            162 STORE_FAST              11 (income_pipeline)\n",
      "\n",
      " 21         164 LOAD_FAST               11 (income_pipeline)\n",
      "            166 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "saved = dis.dis(pipeline_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use module inspect to convert function codes into Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func = inspect.getsource(pipeline_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def pipeline_test_4(f_path = 'adult-sample.csv', a = 0):\\n    raw_data = pd.read_csv(f_path, na_values='?')\\n    data = raw_data.dropna()\\n\\n    labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\\n\\n    nested_categorical_feature_transformation = Pipeline(steps=[\\n        ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\\n        ('encode', OneHotEncoder(handle_unknown='ignore'))\\n    ])\\n\\n    nested_feature_transformation = ColumnTransformer(transformers=[\\n        ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\\n        ('numeric', StandardScaler(), ['age', 'hours-per-week'])\\n    ])\\n\\n    nested_pipeline = Pipeline([\\n      ('features', nested_feature_transformation),\\n      ('classifier', DecisionTreeClassifier())])\\n\\n    return nested_pipeline\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func_list = [item[4:].rstrip() for item in raw_func.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"pipeline_test_4(f_path = 'adult-sample.csv', a = 0):\",\n",
       " \"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " '',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " '',\n",
       " 'nested_categorical_feature_transformation = Pipeline(steps=[',\n",
       " \"    ('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\",\n",
       " \"    ('encode', OneHotEncoder(handle_unknown='ignore'))\",\n",
       " '])',\n",
       " '',\n",
       " 'nested_feature_transformation = ColumnTransformer(transformers=[',\n",
       " \"    ('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),\",\n",
       " \"    ('numeric', StandardScaler(), ['age', 'hours-per-week'])\",\n",
       " '])',\n",
       " '',\n",
       " 'nested_pipeline = Pipeline([',\n",
       " \"  ('features', nested_feature_transformation),\",\n",
       " \"  ('classifier', DecisionTreeClassifier())])\",\n",
       " '',\n",
       " 'return nested_pipeline',\n",
       " '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_func_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For_if_statement Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bracket_balance(func_list):\n",
    "#     res = []\n",
    "#     # func_args = raw_func_list[0].split('(')[1].rstrip('):')\n",
    "#     stack_for_parent = []\n",
    "#     logs_of_parent = []\n",
    "#     for item in func_list:\n",
    "#         logs_of_parent.append(item)\n",
    "#     for char in item:\n",
    "#         if char == '(':\n",
    "#             stack_for_parent.append('(')\n",
    "#         if char == '[':\n",
    "#             stack_for_parent.append('[')\n",
    "#         if char == ')' and stack_for_parent[-1] == '(':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#         if char == ']' and stack_for_parent[-1] == '[':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#     if not stack_for_parent:\n",
    "#         res.append(' '.join(logs_of_parent))\n",
    "#         logs_of_parent.clear()\n",
    "#     return res\n",
    "\n",
    "# logs_of_loop_if = []\n",
    "# for item in raw_func_list:\n",
    "#     if item[-1] == ':':\n",
    "#         logs_of_loop_if.append(item)\n",
    "#     if not logs_of_loop_if and item.startwith('    '):\n",
    "#         logs_of_loop_if.append(item)\n",
    "#     else:\n",
    "#         item.strip()\n",
    "#     logs_of_parent.append(item)\n",
    "#     for char in item:\n",
    "#         if char == '(':\n",
    "#             stack_for_parent.append('(')\n",
    "#         if char == '[':\n",
    "#             stack_for_parent.append('[')\n",
    "#         if char == ')' and stack_for_parent[-1] == '(':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#         if char == ']' and stack_for_parent[-1] == '[':\n",
    "#             stack_for_parent.pop(-1)\n",
    "#     if not stack_for_parent:\n",
    "#         res.append(' '.join(logs_of_parent))\n",
    "#         logs_of_parent.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Function to Convert Function to Executable Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_aggregation(func_str):\n",
    "    '''\n",
    "    This function is used for line execution with exec()\n",
    "    \n",
    "    args:\n",
    "        function strings after inspect\n",
    "    returns:\n",
    "        list of functionable strings for exec()\n",
    "    '''\n",
    "    \n",
    "    res = [] # executables for return\n",
    "    stack_for_parent = [] # stack storing brackets for line integration\n",
    "    logs_of_parent = [] # logs of lines for concat\n",
    "#     convert function codes to list of strings\n",
    "    func_list = [item.strip() for item in func_str.split('\\n')]\n",
    "#     function args\n",
    "    func_args = [item.strip() for item in func_list[0].split('(')[1].rstrip('):').split(',')]\n",
    "    for item in func_list[1:]:\n",
    "        if not item:\n",
    "            continue\n",
    "        logs_of_parent.append(item)\n",
    "        for char in item:\n",
    "            if char == '(':\n",
    "                stack_for_parent.append('(')\n",
    "            if char == '[':\n",
    "                stack_for_parent.append('[')\n",
    "            if char == ')' and stack_for_parent[-1] == '(':\n",
    "                stack_for_parent.pop(-1)\n",
    "            if char == ']' and stack_for_parent[-1] == '[':\n",
    "                stack_for_parent.pop(-1)\n",
    "        if not stack_for_parent:\n",
    "            res.append(''.join(logs_of_parent))\n",
    "            logs_of_parent.clear()\n",
    "    return func_args, res[:-1], [item.strip() for item in res[-1].replace('return ', '').split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c41073d0bdf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutable_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_aggregation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "input_args, executable_list = func_aggregation(raw_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ceac205bc478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_args' is not defined"
     ]
    }
   ],
   "source": [
    "input_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " \"feature_transformation = sklearn.compose.ColumnTransformer(transformers=[('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\",\n",
       " \"income_pipeline = Pipeline([('features', feature_transformation),('classifier', DecisionTreeClassifier())])\"]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_args:\n",
    "    exec(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['race', 'occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_it = iter(executable_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "for _ in range(4):\n",
    "    cur_line = next(exec_it)\n",
    "    exec(cur_line)\n",
    "    try: \n",
    "        if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "            print('yes')\n",
    "            target_df = cur_line.split('=')[0].strip()\n",
    "            exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")\n",
    "        else:\n",
    "            exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")\n",
    "    except:\n",
    "        exec(f\"pprint.pprint({target_df}[{target_col}].describe())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 3 debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"feature_transformation = sklearn.compose.ColumnTransformer(transformers=[('categorical', OneHotEncoder(handle_unknown='ignore'), ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar = cur_line.split('=')[0].strip()\n",
    "str(eval(f\"type({tar})\"))==\"<class 'pandas.core.frame.DataFrame'>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_graph = pipeline_to_dataflow_graph(income_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], (name=features__categorical__education, op=OneHotEncoder),\n",
       " [], (name=features__categorical__workclass, op=OneHotEncoder),\n",
       " [], (name=features__numeric__age, op=StandardScaler),\n",
       " [], (name=features__numeric__hours-per-week, op=StandardScaler),\n",
       " [[], (name=features__categorical__education, op=OneHotEncoder), [], (name=features__categorical__workclass, op=OneHotEncoder), [], (name=features__numeric__age, op=StandardScaler), [], (name=features__numeric__hours-per-week, op=StandardScaler)], (name=classifier, op=DecisionTreeClassifier)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 4 starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_func_4 = inspect.getsource(pipeline_test_4)\n",
    "\n",
    "input_args, executable_list, outputs = func_aggregation(raw_func_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_args:\n",
    "    exec(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"raw_data = pd.read_csv(f_path, na_values='?')\",\n",
       " 'data = raw_data.dropna()',\n",
       " \"labels = label_binarize(data['income-per-year'], ['>50K', '<=50K'])\",\n",
       " \"nested_categorical_feature_transformation = Pipeline(steps=[('impute', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),('encode', OneHotEncoder(handle_unknown='ignore'))])\",\n",
       " \"nested_feature_transformation = ColumnTransformer(transformers=[('categorical', nested_categorical_feature_transformation, ['education', 'workclass']),('numeric', StandardScaler(), ['age', 'hours-per-week'])])\",\n",
       " \"nested_pipeline = Pipeline([('features', nested_feature_transformation),('classifier', DecisionTreeClassifier())])\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executable_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nested_pipeline']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['race', 'occupation', 'education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race          100\n",
      "occupation     94\n",
      "education     100\n",
      "dtype: int64\n",
      "Inpected raw_data = pd.read_csv(f_path, na_values='?')\n",
      "-------------------------------------------------------\n",
      "\n",
      "Count Changed in race with value -8\n",
      "Count Changed in occupation with value -2\n",
      "Count Changed in education with value -8\n",
      "race          92\n",
      "occupation    92\n",
      "education     92\n",
      "dtype: int64\n",
      "Inpected data = raw_data.dropna()\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev = None\n",
    "for cur_line in executable_list:\n",
    "    print_bool = False\n",
    "    exec(cur_line)\n",
    "    try: \n",
    "        if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "            target_df = cur_line.split('=')[0].strip()\n",
    "            count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "            if prev is not None:\n",
    "                for col in target_col:\n",
    "                    dif = count_log[col] - prev[col]\n",
    "                    if dif != 0:\n",
    "                        print(f'Count Changed in {col} with value {dif}')\n",
    "                        print_bool = True\n",
    "            else:\n",
    "                print_bool = True\n",
    "            \n",
    "            if print_bool:\n",
    "                pprint.pprint(count_log)\n",
    "                print(f'Inpected {cur_line}')\n",
    "                print('-------------------------------------------------------')\n",
    "                print()\n",
    "            prev = count_log\n",
    "            \n",
    "        elif str(eval(f\"type({cur_line.split('=')[0].strip()})\")).startswith(\"<class 'sklearn\"):\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "            count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "            for col in target_col:\n",
    "                dif = count_log[col] - prev[col] \n",
    "                if dif != 0:\n",
    "                    print(f'Count Changed in {col} with value {dif}')\n",
    "                    print_bool = True\n",
    "            if print_bool:\n",
    "                pprint.pprint(count_log)\n",
    "                print(f'Inspected {cur_line}')\n",
    "                print('-------------------------------------------------------')\n",
    "                print()\n",
    "            prev = count_log\n",
    "            \n",
    "    except:\n",
    "#         print(f'inspecting {cur_line}')\n",
    "#         exec(f\"pprint.pprint({target_df}[{target_col}].count())\")\n",
    "#         print()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsf\n"
     ]
    }
   ],
   "source": [
    "if not 0:\n",
    "    print('dsf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_graph = pipeline_to_dataflow_graph(eval(f'{outputs[0]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], (name=education, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)),\n",
       " [[], (name=education, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=education, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)),\n",
       " [], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)),\n",
       " [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=workclass, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)),\n",
       " [], (name=age, op=StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " [], (name=hours-per-week, op=StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       " [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0)), [[], (name=workclass, op=SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "               missing_values=nan, strategy='most_frequent', verbose=0))], (name=workclass, op=OneHotEncoder(categorical_features=None, categories=None, drop=None,\n",
       "               dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "               n_values=None, sparse=True)), [], (name=age, op=StandardScaler(copy=True, with_mean=True, with_std=True)), [], (name=hours-per-week, op=StandardScaler(copy=True, with_mean=True, with_std=True))], (name=, op=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=None, splitter='best'))]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count          92\n",
      "unique         12\n",
      "top       HS-grad\n",
      "freq           29\n",
      "count                                                    92\n",
      "unique                                                    1\n",
      "top         (0, 11)\\t1.0\\n  (1, 8)\\t1.0\\n  (2, 7)\\t1.0\\n...\n",
      "freq                                                     92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for item in nested_graph:\n",
    "    if item.name in target_col:\n",
    "        eval(target_df)[item.name] = item.operation.fit_transform(eval(target_df)[item.name].values.reshape(-1,1))\n",
    "        print(eval(target_df)[item.name].describe().to_string())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_ver(pipeline_to_test, target_col = ['race', 'occupation', 'education']):\n",
    "    \n",
    "    def func_aggregation(func_str):\n",
    "        '''\n",
    "        This function is used for line execution with exec()\n",
    "\n",
    "        args:\n",
    "            function strings after inspect\n",
    "        returns:\n",
    "            list of functionable strings for exec()\n",
    "        '''\n",
    "\n",
    "        res = [] # executables for return\n",
    "        stack_for_parent = [] # stack storing brackets for line integration\n",
    "        logs_of_parent = [] # logs of lines for concat\n",
    "    #     convert function codes to list of strings\n",
    "        func_list = [item.strip() for item in func_str.split('\\n')]\n",
    "    #     function args\n",
    "        func_args = [item.strip() for item in func_list[0].split('(')[1].rstrip('):').split(',')]\n",
    "        for item in func_list[1:]:\n",
    "            if not item:\n",
    "                continue\n",
    "            logs_of_parent.append(item)\n",
    "            for char in item:\n",
    "                if char == '(':\n",
    "                    stack_for_parent.append('(')\n",
    "                if char == '[':\n",
    "                    stack_for_parent.append('[')\n",
    "                if char == ')' and stack_for_parent[-1] == '(':\n",
    "                    stack_for_parent.pop(-1)\n",
    "                if char == ']' and stack_for_parent[-1] == '[':\n",
    "                    stack_for_parent.pop(-1)\n",
    "            if not stack_for_parent:\n",
    "                res.append(''.join(logs_of_parent))\n",
    "                logs_of_parent.clear()\n",
    "        return func_args, res[:-1], [item.strip() for item in res[-1].replace('return ', '').split(',')]\n",
    "    \n",
    "    \n",
    "    raw_func = inspect.getsource(pipeline_to_test)\n",
    "\n",
    "    input_args, executable_list, outputs = func_aggregation(raw_func)\n",
    "    \n",
    "    for line in input_args:\n",
    "        exec(line)\n",
    "    \n",
    "    print()\n",
    "    print('####################### Start Pandas Opeation #######################')\n",
    "    print()\n",
    "    \n",
    "    prev = None\n",
    "    for cur_line in executable_list:\n",
    "        print_bool = False\n",
    "        exec(cur_line)\n",
    "        try: \n",
    "            if str(eval(f\"type({cur_line.split('=')[0].strip()})\")) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
    "                target_df = cur_line.split('=')[0].strip()\n",
    "                count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "                if prev is not None:\n",
    "                    for col in target_col:\n",
    "                        dif = count_log[col] - prev[col]\n",
    "                        if dif != 0:\n",
    "                            print(f'Count Changed in {col} with value {dif}')\n",
    "                            print_bool = True\n",
    "                else:\n",
    "                    print_bool = True\n",
    "\n",
    "                if print_bool:\n",
    "                    pprint.pprint(count_log)\n",
    "                    print(f'Inpected {cur_line}')\n",
    "                    print('-------------------------------------------------------')\n",
    "                    print()\n",
    "                prev = count_log\n",
    "\n",
    "            elif str(eval(f\"type({cur_line.split('=')[0].strip()})\")).startswith(\"<class 'sklearn\"):\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                count_log = eval(f\"{target_df}[{target_col}].count()\")\n",
    "                for col in target_col:\n",
    "                    dif = count_log[col] - prev[col] \n",
    "                    if dif != 0:\n",
    "                        print(f'Count Changed in {col} with value {dif}')\n",
    "                        print_bool = True\n",
    "                if print_bool:\n",
    "                    pprint.pprint(count_log)\n",
    "                    print(f'Inspected {cur_line}')\n",
    "                    print('-------------------------------------------------------')\n",
    "                    print()\n",
    "                prev = count_log\n",
    "\n",
    "        except:\n",
    "    #         print(f'inspecting {cur_line}')\n",
    "    #         exec(f\"pprint.pprint({target_df}[{target_col}].count())\")\n",
    "    #         print()\n",
    "            pass\n",
    "    \n",
    "    nested_graph = pipeline_to_dataflow_graph(eval(f'{outputs[0]}'))\n",
    "    \n",
    "    print()\n",
    "    print('####################### Start Sklearn Pipeline #######################')\n",
    "    print()\n",
    "    \n",
    "    for item in nested_graph:\n",
    "        if item.name in target_col:\n",
    "            eval(target_df)[item.name] = item.operation.fit_transform(eval(target_df)[item.name].values.reshape(-1,1))\n",
    "            print(f\"Operations {str(item.operation).split('(')[0]} on {item.name}\")\n",
    "            count_log = eval(target_df)[item.name].count()\n",
    "            dif = count_log - prev[item.name]\n",
    "            if dif != 0:\n",
    "                print(f\"count now is {count_log}, dif is {dif}\")\n",
    "            else:\n",
    "                print('no changes')\n",
    "            prev[item.name] = count_log\n",
    "            print('-------------------------------------------------------')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################### Start Pandas Opeation #######################\n",
      "\n",
      "\n",
      "####################### Start Sklearn Pipeline #######################\n",
      "\n",
      "Operations SimpleImputer on education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huangbiao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ea56ca72c486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribe_ver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_test_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-da46c2095d93>\u001b[0m in \u001b[0;36mdescribe_ver\u001b[0;34m(pipeline_to_test, target_col)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Operations {str(item.operation).split('(')[0]} on {item.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mcount_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_log\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdif\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"count now is {count_log}, dif is {dif}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "describe_ver(pipeline_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
